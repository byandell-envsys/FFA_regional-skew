---
title: "Regional skew coefficient analysis "
subtitle: "Clean data
author: "CJ Tinant"
date: "`r Sys.Date()`"
output: html_document
---

<!--
Purpose:

Overview:
* load and process spatial data -- currently only locally works
-- imports and validates shapefiles
-- returns spatial objects with centroid and coordinates

* iteratively visualize and select ecoregions



# Next steps:

# References:
https://github.com/EmilHvitfeldt/r-color-palettes
https://r.geocompx.org/
-->


```{r 00_setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

# library
library(tidyverse)        # Load the 'Tidyverse' packages: ggplot2, dplyr, 
                          #   tidyr, readr, purrr, tibble, stringr, and forcats
library(dataRetrieval)    # Retrieval functions for USGS and EPA hydrology and
                          #   water quality data
library(sf)               # Simple features for R
library(here)

# Functions:
# process_geometries -- written with ChatGPT 4.0 on 2024-06-06
#     The function returns a modified sf object with additional columns for
#     the centroid and its coordinates.
# The function automates the following functions:
#     Check and make geometries valid
#         eco_l3$geometry <- st_make_valid(eco_l3$geometry)
#     Safely calculate centroids for valid geometries
#         eco_l3$centroid <- ifelse(st_is_valid(eco_l3$geometry),
#                                   st_centroid(eco_l3$geometry), NA)
#      Extract coordinates from centroids
#          eco_l3$text_x <- ifelse(!is.na(eco_l3$centroid),
#                                  st_coordinates(eco_l3$centroid)[, 1],
#                                  NA)
#          eco_l3$text_y <- ifelse(!is.na(eco_l3$centroid),
#                                  st_coordinates(eco_l3$centroid)[, 2],
#                                  NA)
#
# Details about process_geometries:
# Geometry Validation:
#   st_make_valid() is used to correct any invalid geometries within the
#     spatial data frame. The function identifies indices of valid geometries
#     to ensure that centroids and subsequent operations are only applied to
#     them.
# Centroid Calculation:
#   st_centroid() is applied conditionally only to valid geometries using
#     ifelse().
#   If a geometry is invalid, NA is used as a fallback.
#      st_centroid() is applied only to the valid parts of the geometry column.
#   Centroids are stored in a list to handle any type of geometry being returned
#      from st_centroid().
#   Each geometry is processed individually within a loop to allow more control
#      over handling each item and better debugging capabilities if errors occur.
#   !is_empty(centroid) checks if the centroid is not empty. 
#   The function also ensures that st_coordinates(centroid) actually returns
#      a non-empty data frame before trying to access its elements.
# Conditional Coordinates Extraction:
#   The x and y coordinates are extracted from the centroid. If the centroid
#      is NA (because the geometry was invalid), the coordinate fields are set
#      to NA.
#   Coordinates are extracted only if there are valid centroids. This is
#      safeguarded by checking if there are valid indices before attempting to
#      extract coordinates.
#   text_x and text_y are initialized with NA_real_ to ensure that the type
#      consistency is maintained for cases where centroids might not be
#      computable.
#   Before extracting coordinates, the function checks if the centroid is not NA
#      and contains rows. Then it ensures that the coordinates can be indexed
#      properly, and has the required number of columns 
#        (at least two, for x and y coordinates).
# Additional checks:
#   Separate Checks for NAs and Data Structure Validity: The function checks
#       for NAs and the structure of coords are now more explicit.
#     The function checks if centroid is not NA and not empty. Then, if coords
#       is derived, the function ensures it is not NA and has the necessary rows
#       and columns.
#   Avoid Coercion Errors: By ensuring each part of the conditional is valid
#       before evaluating the next part, this prevents logical operations on
#       possibly undefined or inappropriate data types.
#   Direct Evaluation of Conditions: The logic is structured to progressively
#       verify conditions before accessing potentially problematic attributes
#       like the number of rows or columns.
#   Check for null in coords: The function ensures that coords is not null
#       before proceeding to check its dimensions. This prevents logical errors
#       when coords might be an unexpected type or structure.
#   Explicit Structure Check: By using is.null along with checks for the number
#       of rows and columns in coords, the function can more reliably ensure
#       that the data structure is correct before attempting to access its
#       elements.

process_geometries <- function(sf_object) {
  # Ensure all geometries are valid
  sf_object$geometry <- st_make_valid(sf_object$geometry)
  
  # Initialize columns for centroids and coordinates
  sf_object$text_x <- rep(NA_real_, nrow(sf_object))
  sf_object$text_y <- rep(NA_real_, nrow(sf_object))

  # Calculate centroids for valid geometries and extract coordinates
  for (i in seq_len(nrow(sf_object))) {
    if (st_is_valid(sf_object$geometry[i])) {
      centroid <- st_centroid(sf_object$geometry[i])
      if (!is.na(centroid) && !st_is_empty(centroid)) {
        coords <- st_coordinates(centroid)
        # Explicit check for coords' validity and structure
        if (!is.null(coords) && nrow(coords) > 0 && ncol(coords) >= 2) {
          sf_object$text_x[i] <- coords[1, 1]
          sf_object$text_y[i] <- coords[1, 2]
        }
      }
    }
  }

  # Return the modified sf object
  return(sf_object)
}

```

## Get data -- either from Cyverse or locally

```{r 01a_get_and_clean_data_local, eval=FALSE}

# load shapefiles -- local path -- and check geometry
file_path <- "data_spatial/"

# usa ecoregions level 3
file_name <- "us_eco_l3/us_eco_l3.shp"
eco_l3 <- st_read(paste(file_path,
                        file_name,
                        sep = "")) %>%
  janitor::clean_names() %>%
  process_geometries()

# usa ecoregions level 4 -- no state boundaries
file_name <- "us_eco_l4/us_eco_l4_no_st.shp"
eco_l4 <- st_read(paste(file_path,
                        file_name,
                        sep = "")) %>%
  janitor::clean_names() %>%
  process_geometries()

# usa states
file_name <- "tl_2012_us_state/tl_2012_us_state.shp"
state_bdy_usa <- st_read(paste(file_path,
                        file_name,
                        sep = "")) %>%
  janitor::clean_names() %>%
  process_geometries()

# usa tribal lands
file_name <- "tl_2020_us_aitsn/tl_2020_us_aitsn.shp"
tribal_lands_usa <- st_read(paste(file_path,
                        file_name,
                        sep = "")) %>%
  janitor::clean_names() %>%
  process_geometries()

# census codes for American Indian, Alaska Native, and
#   Native Hawaiian Areas (AIANNH) 2020 ----
#     https://www.census.gov/library/reference/code-lists/ansi.html
#   need to pivot the data because tribal lands in multiple states
file_path <- "data_spatial/metadata/"
file_name <- "national_aiannh2020.csv"

aiannh_table <- read_csv(paste(file_path, file_name, sep = "")) %>%
  janitor::clean_names()

# clean up Global Environment
rm(list = ls(pattern = "file"))
rm(list = ls(pattern = "process"))

```

```{r 01b_get_and_clean_data_Cyverse, eval=FALSE}

# load shapefiles -- local path -- and check geometry
eco_l3 <- st_read("data_spatial/us_eco_l3/us_eco_l3.shp") %>%
  janitor::clean_names() %>%
  process_geometries()

eco_l4 <- st_read("data_spatial/us_eco_l4/us_eco_l4_no_st.shp") %>%
  janitor::clean_names() %>%
  process_geometries()

state_bdy <- st_read("data_spatial/tl_2012_us_state/tl_2012_us_state.shp") %>%
  janitor::clean_names() %>%
  process_geometries()

tribal_lands_usa <- st_read("data_spatial/tl_2020_us_aitsn/tl_2020_us_aitsn.shp") %>%
  janitor::clean_names() %>%
  process_geometries()

# load 2020 Census codes for American Indian, Alaska Native, and
#   Native Hawaiian Areas (AIANNH)----
#     https://www.census.gov/library/reference/code-lists/ansi.html
#   need to pivot the data because tribal lands in multiple states
file_path <- "data_spatial/metadata/"
file_name <- "national_aiannh2020.csv"

aiannh_table <- read_csv(paste(file_path, file_name, sep = "")) %>%
  janitor::clean_names() %>%
  separate_wider_delim(
    cols = states,
    delim = "~",
    names = c("scratch_1", "scratch_2", "scratch_3"),
    too_few = "align_start",
    too_many = "merge"
    ) %>%
  pivot_longer(cols = starts_with("scratch"),
               names_to = "scratch",
               values_to = "states") %>%
  select(-scratch) %>%
  filter(!is.na(states))

```

## Visualize ecoregions and select study area

```{r 02a_intersect_eco_l3_bdry_w_rm_gp_region}

# This code chunk intersects eco_l3 with the RM and GP region boundary for
#   Indian Lands of Federally Recognized Tribes of the United States
#   1. filter the tribal lands in Rocky Mountain and Great Plains regions
#         start with selecting census codes for states in RM and GP ecoregions;
#   2. check shapefiles geometry
#   3. Reproject ecoregion shapefile to NAD83 Geographic (ESPG:4269) -- Lat Lon
#   4. Filter States in Rocky Mountain and Great Plains regions
#   5. Make a single polygon of the Rocky Mountain and Great Plains regions

# select the census codes for states in Rocky Mountain and Great Plains regions
aiannh_rm_gp <- aiannh_table %>%
  filter(states == "SD" |
         states == "ND" |
         states == "NE" |
         states == "MT" |
         states == "WY"
           )

# filter the tribal lands in Rocky Mountain and Great Plains regions
tribal_lands_rm_gp <- tribal_lands_usa %>%
  filter(aiannhce %in% aiannh_rm_gp$aiannhce)

# check geometry of shapefiles
crs_geom_eco <- eco_l3 %>%
  st_crs() %>%
  pluck(.,1)

crs_geom_st_bdy_usa <- state_bdy_usa %>%
  st_crs() %>%
  pluck(.,1)

# reproject ecoregions to NAD83 Geographic (ESPG:4269) -- Lat Lon
eco_l3_proj <- eco_l3 %>%
  st_transform(crs = 4269)

# filter States in Rocky Mountain and Great Plains regions
state_bdy <- state_bdy_usa %>%
  filter(stusps == "SD" |
         stusps == "ND" |
         stusps == "NE" |
         stusps == "MT" |
         stusps == "WY"
           )

# make a single polygon of the Rocky Mountain and Great Plains regions
rm_gp_bdy <- state_bdy %>%
  st_union()

# intersect eco_l3 by the Rocky Mountain and Great Plains region boundary
eco_l3_rm_gp <- st_intersection(eco_l3_proj, rm_gp_bdy)

# clean up Global Environment
rm(aiannh_table,
   eco_l3_proj,
   eco_l3,
   state_bdy_usa,
   tribal_lands_usa,
   crs_geom_eco,
   crs_geom_st_bdy
   )

```


```{r 02b_visualize_eco_l3_bdry_w_rm_gp_region}

# orig data
# make a quick plot
# ggplot() +
#   geom_sf(data = eco_l3_rm_gp,
#           aes(fill = us_l3name,)) +
#   geom_sf(data = state_bdy,
#           fill = "transparent")

# cleaner theme
ggplot() +
  geom_sf(data = eco_l3_rm_gp,    # Fill regions based on ecological names
          aes(fill = us_l3name)
          ) +  
  geom_sf(data = state_bdy,       # State boundaries with no fill
          fill = NA,
          color = "black",
          size = 0.5
          ) +
  labs(title = "Ecological Regions with State Boundaries",
       fill = "Ecological Region") +  # Adding title and legend title
  scale_fill_viridis_d() +  # A color scale that's visually appealing and accessible
  theme_minimal()  # A clean theme

```

```{r 02c_visually_explore_l1_great-plains}

# Select the Great Plains Level 1 ecoregion
eco_l3_gp <- eco_l3_rm_gp %>%
  filter(na_l1name == "GREAT PLAINS")

# Plot with text labels
# ggplot() +
#   geom_sf(data = eco_l3_gp,
#           aes(fill = us_l3name)) +
#   geom_sf(data = state_bdy,
#           fill = "transparent")

# cleaner theme
ggplot() +
  geom_sf(data = eco_l3_gp,    # Fill regions based on ecological names
          aes(fill = us_l3name)
          ) +  
  geom_sf(data = state_bdy,       # State boundaries with no fill
          fill = NA,
          color = "black",
          size = 0.5
          ) +
  labs(title = "Great Plains Ecoregions with State Boundaries",
       fill = "Ecological Region") +  # Adding title and legend title
  scale_fill_viridis_d() +  # A color scale that's visually appealing and accessible
  theme_minimal()  # A clean theme

```

```{r 02d_visually_explore_l2_semi-arid-prairies}

# Select the Great Plains Level 1 ecoregion
eco_l3_semi <- eco_l3_rm_gp %>%
  filter(na_l2name == "SOUTH CENTRAL SEMI-ARID PRAIRIES" |
         na_l2name == "WEST-CENTRAL SEMI-ARID PRAIRIES")

# Plot with text labels
# ggplot() +
#   geom_sf(data = eco_l3_semi,
#           aes(fill = us_l3name)) +
#   geom_sf(data = state_bdy,
#           fill = "transparent") +
#   geom_sf(data = tribal_lands_rm_gp,
#           aes(fill = aiannhce),
#           show.legend = c(fill = FALSE))

# cleaner theme
ggplot() +
  geom_sf(data = eco_l3_semi,    # Fill regions based on ecological names
          aes(fill = us_l3name)
          ) +  
  geom_sf(data = state_bdy,       # State boundaries with no fill
          fill = NA,
          color = "black",
          size = 0.5
          ) +
  labs(title = "Semi-Arid Great Plains Ecoregions with State Boundaries",
       fill = "Ecological Region") +  # Adding title and legend title
  scale_fill_viridis_d() +  # A color scale that's visually appealing and accessible
  theme_minimal()  # A clean theme

```

```{r 02e_visually_explore_l3_semi-arid-prairies_not_glaciated}

# Select the Great Plains Level 1 ecoregion
eco_l3_semi_ng <- eco_l3_semi %>%
  filter(na_l3name != "Northwestern Glaciated Plains")

# # Plot with text labels
# ggplot() +
#   geom_sf(data = eco_l3_semi_ng,
#           aes(fill = us_l3name,
#               show.legend = FALSE)
#           ) +
#   geom_sf(data = state_bdy,
#           fill = "transparent") +
#   geom_sf(data = tribal_lands_rm_gp,
#           alpha = 0.5) +
#   theme_minimal() +
#   ggtitle("Map of level-3 ecoregions") 

# cleaner theme
ggplot() +
  geom_sf(data = eco_l3_semi_ng,    # Fill regions based on ecological names
          aes(fill = us_l3name)
          ) +  
  geom_sf(data = state_bdy,       # State boundaries with no fill
          fill = NA,
          color = "black",
          size = 0.5
          ) +
  labs(title = "Non-glaciated Semi-Arid Great Plains Ecoregions",
       fill = "Ecological Region") +  # Adding title and legend title
  scale_fill_viridis_d() +  # A color scale that's visually appealing and accessible
  theme_minimal()  # A clean theme

```

## REDO BELOW
```{r xx_get_and_clean_data_Cyverse, eval=FALSE}

sites_pk_bb <- read_csv("~/FFA_regional-skew/data/sites_pk_bb.csv")

# load ecoregion shapefile -- path for Cyverse 
eco_l3 <- st_read("~/data-store/home/cjtinant/data_spatial/us_eco_l3.shp") %>%
  janitor::clean_names()

```


```{r}

# get site data----
sites_pk_bb <- read_csv("data/sites_pk_bb.csv") # unclear where this is from

```


```{r 02_convert_to_spatial_reproject_data}

# convert stations into a spatial format (sf) object
sites_pk_bb <- st_as_sf(sites_pk_bb,
                    coords = c("dec_long_va",        # note x goes first
                                "dec_lat_va"),
                    crs = 4269,                     # projection, this is NAD83
                    remove = FALSE)                 # don't remove lat/lon cols

# check geometry of shapefiles
crs_geom <- eco_l3 %>%
  st_crs() %>%
  pluck(.,1)

crs_geom <- eco_l4 %>%
  st_crs() %>%
  pluck(.,1)

crs_geom <- state_bdy %>%
  st_crs() %>%
  pluck(.,1)

# reproject ecoregions to NAD83 Geographic (ESPG:4269) -- Lat Lon
eco_l3 <- eco_l3 %>%
  st_transform(crs = 4269)

eco_l4 <- eco_l4 %>%
  st_transform(crs = 4269)

```

```{r eval=FALSE}

eco_l3 %>%
  filter(na_l3name == "High Plains" |
         na_l3name == "Northwestern Great Plains" |
         na_l3name == "Nebraska Sand Hills") %>%
  ggplot() +
  geom_sf()




```

```{r 04_reproject_eco_shapefile}

# check geometry of ecoreg
crs_eco_l3 <- eco_l3 %>%
  st_crs()

# subset study area -- Albers refers to Albers Equal Area Conic projection
#   * High Plains
study_area_albers <- eco_l3 %>%
  filter(na_l3name == "High Plains" |
         na_l3name == "Northwestern Great Plains")

# project study area to NAD83 Geographic (ESPG:4269) -- Lat Lon
#   need to keep the study area
study_area_unproj <- study_area_albers %>%
  st_transform(crs = 4269)

# check results
crs_study_area_unproj <- study_area_unproj %>%
  st_crs()

```

```{r 05_reproject_eco_shapefile}
# make bounding box -- done recursively to create a bb in 01_get_ecoreg_shapefiles
bb_study_area_unproj <- study_area_unproj %>%
  st_bbox() %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  pivot_longer(-rowname) %>%
  pivot_wider(names_from = rowname,
              values_from = value) %>%
  select(-name) %>%
  mutate(delta_x = xmax - xmin) %>%
  mutate(delta_y = ymax - ymin)

# clean up Global Environment
rm(list = ls(pattern = "names"))
rm(list = ls(pattern = "crs"))
#rm(eco_l3)

```

```{r 06_intersect_sites_study_area}

# drop sites outside of ecoreg
sites_pk_in_ecoreg <- st_intersection(sites_pk_bb, study_area_unproj)

# filter sites with greater than 20 years of data
sites_gt_20 <- sites_pk_in_ecoreg %>%
  filter(count_nu >= 20) %>%
  distinct()

# check for duplicates
duplicates <- sites_gt_20 %>%
  filter(duplicated(.) | duplicated(., fromLast = TRUE))

# clean up Global Environment
rm(bb_study_area_unproj)

```

```{r 07_get_peak-flow_data_w_min_20_obs}

# get peak flow data for gages with min 20 observations
site_ids <- sites_gt_20 %>%
  select(site_no) %>%
  distinct()

peak_data_gt_20 <- readNWISpeak(site_ids$site_no)

# check results
ck_sites_gt_20 <- nrow(peak_data_gt_20 %>%
  group_by(site_no) %>%
  summarise(count = n())) == nrow(sites_gt_20)

duplicates <- peak_data_gt_20 %>%
  filter(duplicated(.) | duplicated(., fromLast = TRUE))

# remove duplicates
peak_data_no_dups <- peak_data_gt_20 %>%
  distinct()

# clean up Global Environment
rm(site_ids,
   ck_sites_gt_20,
   duplicates,
   peak_data_gt_20
   )

```

```{r 08_visually_check_results}

# plot results
ggplot() +
  geom_sf(data = study_area_unproj,
          alpha = 0.3) +
  geom_sf(data = sites_pk_bb,
          size  =  0.1,
          color = "gray80",
          alpha = 0.3) +
  geom_sf(data = sites_pk_in_ecoreg,
          size = 0.2,
          color = "red",
          alpha = 0.4) +
  geom_sf(data = sites_gt_20,
          size = 0.4,
          color = "black"
          ) +
  theme_bw() +
labs(
  title = "Initial gage selection",
  subtitle = "Peak flow gages with at least 20 years of record"
)

```

```{r 08_export_results}

# save plot of gages
ggsave("figures/gage-selection_initial.png")

# save site-data as a csv
write_csv(sites_pk_in_ecoreg, "data/sites_pk_in_ecoreg.csv")

write_csv(sites_gt_20, "data/sites_peak_gt_20.csv")

# save peak flow data as a csv
write_csv(peak_data_no_dups, "data/data_peak_gt_20.csv")

# Clean up Global Environment
rm(sites_pk_bb,
   sites_pk_in_ecoreg,
   study_area_albers,
   study_area_unproj,
   bb_study_area_unproj,
   )

```


```{r old-visualization-code_to-add_check}

# ggplot() +
#   geom_sf(data = eco_l3_semi_ng,
#           aes(fill = us_l3name)#,
#           #show.legend = FALSE
#           ) +  # Control legend visibility globally or modify if needed
#   geom_sf(data = state_bdy,
#           fill = "transparent",
#           color = "black",         # Add a border color for states
#           size = 0.5) +            # Adjust line size
#   geom_sf(data = tribal_lands_rm_gp,
#           fill = "gray",           # Adjust color for visibility
#           alpha = 0.5) +
#   scale_fill_viridis_d(option = "C") +  # Use a colorblind-friendly palette
#   labs(title = "Map of Level-3 Ecoregions",
#        caption = "Data source: XYZ") +
#   theme_minimal() +
#   theme(legend.position = "bottom",    # Adjust legend position
#         plot.title = element_text(hjust = 0.5),
#         plot.caption = element_text(hjust = 0, size = 8),
#         axis.text = element_blank(),
#         axis.ticks = element_blank())
# 
# ggsave("data_spatial/ecoregions_map_final.png",
#        width = 10,
#        height = 8,
#        bg = "white",
#        dpi = 300)  # Save high-resolution image


# check table and spatial data prior to join
#   make a quick check of the data
# ch_table <- aiannh_table %>%
#   filter(states == "SD" |
#          states == "ND")
# 
# ch_spatial <- tribal_lands_usa %>%
#   filter(aiannhce %in% ch_table$aiannhce)
# 
# # join metadata to spatial data
# tribal_lands_join <- right_join(aiannh_table, tribal_lands_usa,
#                            by = join_by(aiannhce))

# fix the table
 # %>%
 #  separate_wider_delim(
 #    cols = states,
 #    delim = "~",
 #    names = c("scratch_1", "scratch_2", "scratch_3"),
 #    too_few = "align_start",
 #    too_many = "merge"
 #    ) %>%
 #  pivot_longer(cols = starts_with("scratch"),
 #               names_to = "scratch",
 #               values_to = "states") %>%
 #  select(-scratch) %>%
 #  filter(!is.na(states))


# # filter tribal lands in Rocky Mountain and Great Plains regions
# tribal_lands_rm_gp <- tribal_lands_join %>%
#   filter(states == "SD" |
#          states == "ND" |
#          states == "NE" |
#          states == "MT" |
#          states == "WY"
#            ) %>%
#   process_geometries()
# 
# # clean up Global Environment
# rm(list = ls(pattern = "ch"))
# rm(aiannh_table)

```

```{r eval=FALSE}

#%>%
  filter(region != 9)   %>%     # eliminates island protectorates
  filter(division > 3)  %>%     # eliminates northeastern states
  filter(division != 5) %>%     # eliminates eastern states
  filter(division != 7) #%>%     # eliminates western states
  filter(stusps != "HI" &
         stusps != "GU" &
         stusps != "AS" &
         stusps != "MP" &
         stusps != "VI" &
         stusps != "AK" &
         stusps != "PR"
         )
```




