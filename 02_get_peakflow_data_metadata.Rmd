---
title: "Peak flow data for Northwestern Great Plains and High Plains ecoregion"
author: "CJ Tinant"
date: "`r Sys.Date()`"
output: html_document
---

<!--
Gets all USGS site numbers from a bounding box
Filters by stream, peak flow

# Need to 
-->


```{r 00_setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

# library
library(tidyverse)        # Load the 'Tidyverse' packages: ggplot2, dplyr, 
                          #   tidyr, readr, purrr, tibble, stringr, and forcats
library(dataRetrieval)    # Retrieval functions for USGS and EPA hydrology and
                          #   water quality data
library(sf)               # Simple features for R
library(here)

# Functions:
# process_geometries -- written with ChatGPT 4.0 on 2024-06-06
#     The function returns a modified sf object with additional columns for
#     the centroid and its coordinates.
# The function automates the following functions:
#     Check and make geometries valid
#         eco_l3$geometry <- st_make_valid(eco_l3$geometry)
#     Safely calculate centroids for valid geometries
#         eco_l3$centroid <- ifelse(st_is_valid(eco_l3$geometry),
#                                   st_centroid(eco_l3$geometry), NA)
#      Extract coordinates from centroids
#          eco_l3$text_x <- ifelse(!is.na(eco_l3$centroid),
#                                  st_coordinates(eco_l3$centroid)[, 1],
#                                  NA)
#          eco_l3$text_y <- ifelse(!is.na(eco_l3$centroid),
#                                  st_coordinates(eco_l3$centroid)[, 2],
#                                  NA)
#
# Details about process_geometries:
# Geometry Validation:
#   st_make_valid() is used to correct any invalid geometries within the
#     spatial data frame. The function identifies indices of valid geometries
#     to ensure that centroids and subsequent operations are only applied to
#     them.
# Centroid Calculation:
#   st_centroid() is applied conditionally only to valid geometries using
#     ifelse().
#   If a geometry is invalid, NA is used as a fallback.
#      st_centroid() is applied only to the valid parts of the geometry column.
#   Centroids are stored in a list to handle any type of geometry being returned
#      from st_centroid().
#   Each geometry is processed individually within a loop to allow more control
#      over handling each item and better debugging capabilities if errors occur.
#   !is_empty(centroid) checks if the centroid is not empty. 
#   The function also ensures that st_coordinates(centroid) actually returns
#      a non-empty data frame before trying to access its elements.
# Conditional Coordinates Extraction:
#   The x and y coordinates are extracted from the centroid. If the centroid
#      is NA (because the geometry was invalid), the coordinate fields are set
#      to NA.
#   Coordinates are extracted only if there are valid centroids. This is
#      safeguarded by checking if there are valid indices before attempting to
#      extract coordinates.
#   text_x and text_y are initialized with NA_real_ to ensure that the type
#      consistency is maintained for cases where centroids might not be
#      computable.
#   Before extracting coordinates, the function checks if the centroid is not NA
#      and contains rows. Then it ensures that the coordinates can be indexed
#      properly, and has the required number of columns 
#        (at least two, for x and y coordinates).
# Additional checks:
#   Separate Checks for NAs and Data Structure Validity: The function checks
#       for NAs and the structure of coords are now more explicit.
#     The function checks if centroid is not NA and not empty. Then, if coords
#       is derived, the function ensures it is not NA and has the necessary rows
#       and columns.
#   Avoid Coercion Errors: By ensuring each part of the conditional is valid
#       before evaluating the next part, this prevents logical operations on
#       possibly undefined or inappropriate data types.
#   Direct Evaluation of Conditions: The logic is structured to progressively
#       verify conditions before accessing potentially problematic attributes
#       like the number of rows or columns.
#   Check for null in coords: The function ensures that coords is not null
#       before proceeding to check its dimensions. This prevents logical errors
#       when coords might be an unexpected type or structure.
#   Explicit Structure Check: By using is.null along with checks for the number
#       of rows and columns in coords, the function can more reliably ensure
#       that the data structure is correct before attempting to access its
#       elements.

process_geometries <- function(sf_object) {
  # Ensure all geometries are valid
  sf_object$geometry <- st_make_valid(sf_object$geometry)
  
  # Initialize columns for centroids and coordinates
  sf_object$text_x <- rep(NA_real_, nrow(sf_object))
  sf_object$text_y <- rep(NA_real_, nrow(sf_object))

  # Calculate centroids for valid geometries and extract coordinates
  for (i in seq_len(nrow(sf_object))) {
    if (st_is_valid(sf_object$geometry[i])) {
      centroid <- st_centroid(sf_object$geometry[i])
      if (!is.na(centroid) && !st_is_empty(centroid)) {
        coords <- st_coordinates(centroid)
        # Explicit check for coords' validity and structure
        if (!is.null(coords) && nrow(coords) > 0 && ncol(coords) >= 2) {
          sf_object$text_x[i] <- coords[1, 1]
          sf_object$text_y[i] <- coords[1, 2]
        }
      }
    }
  }

  # Return the modified sf object
  return(sf_object)
}

```

## Get data -- either from Cyverse or locally
```{r 01a_get_and_clean_data_Cyverse, eval=FALSE}

sites_pk_bb <- read_csv("~/FFA_regional-skew/data/sites_pk_bb.csv")

# load ecoregion shapefile -- path for Cyverse 
eco_l3 <- st_read("~/data-store/home/cjtinant/data_spatial/us_eco_l3.shp") %>%
  janitor::clean_names()

```

```{r 01b_get_and_clean_data_local, eval=FALSE}

# load shapefiles -- local path -- and check geometry
eco_l3 <- st_read("data_spatial/us_eco_l3/us_eco_l3.shp") %>%
  janitor::clean_names() %>%
  process_geometries()

eco_l4 <- st_read("data_spatial/us_eco_l4/us_eco_l4_no_st.shp") %>%
  janitor::clean_names() %>%
  process_geometries()

state_bdy <- st_read("data_spatial/tl_2012_us_state/tl_2012_us_state.shp") %>%
  janitor::clean_names() %>%
  process_geometries()

# get site data
sites_pk_bb <- read_csv("data/sites_pk_bb.csv")

```

# Explore data
```{r 02a_explore_eco_l1_great-plains, eval=FALSE}

# Explore level 1 ecoregion names
eco_l1_names <- eco_l3 %>%
  as.data.frame() %>%
  select(na_l1name) %>%
  distinct()

# Select the Great Plains Level 1 ecoregion
eco_l1_great_plains <- eco_l3 %>%
  filter(na_l1name == "GREAT PLAINS")

# Plot with text labels
ggplot() +
  geom_sf(data = eco_l1_great_plains,
          aes(fill = na_l2name),
          alpha = 0.2,
          show.legend = FALSE
          ) +
  geom_sf(data = eco_l1_great_plains,
          aes(fill = na_l3name),
          alpha = 0.2,
          show.legend = FALSE
          ) +
  geom_text(data = eco_l1_great_plains,
            aes(x = text_x,
                y = text_y,
                label = na_l2code
                ),
            size = 3,
            color = "black",
            check_overlap = TRUE) +
  theme_minimal() +
  labs(title = "Map Title",
       fill = "Legend Title",
       caption = "Source: Your Source") +
  theme(legend.position = "none")

# save scratch plot
ggsave("figures/scratch.png",
       bg = "white")

```

```{r 02b_explore_eco_l2_semi-arid, eval=FALSE}

# Explore level 2 ecoregion names in Great Plains
eco_l2_names <- eco_l1_great_plains %>%
  as.data.frame() %>%
  select(c(na_l2code, na_l2name)) %>%
  distinct()

# Select the Great Plains Level 1 ecoregion
eco_l2_semi_arid <- eco_l3 %>%
  filter(na_l2name == "WEST-CENTRAL SEMI-ARID PRAIRIES" |
         na_l2name == "SOUTH CENTRAL SEMI-ARID PRAIRIES")

# Plot with text labels
ggplot() +
  geom_sf(data = eco_l2_semi_arid,
          aes(fill = na_l3name),
          alpha = 0.5,
          show.legend = FALSE
          ) +
  geom_text(data = eco_l2_semi_arid,
            aes(x = text_x,
                y = text_y,
                label = na_l3name
                ),
            size = 3,
            color = "black",
            check_overlap = TRUE) +
  theme_minimal() +
  labs(title = "Map Title",
       fill = "Legend Title",
       caption = "Source: Your Source") +
  theme(legend.position = "none")

# save scratch plot
ggsave("figures/scratch.png",
       bg = "white")

```

# START HERE
```{r 02c_explore_eco_l3_nw-grt-pln_high-pln_sandhills, eval=FALSE}

# start here
# Explore level 2 ecoregion names in Great Plains
eco_l2_names <- eco_l1_great_plains %>%
  as.data.frame() %>%
  select(c(na_l2code, na_l2name)) %>%
  distinct()

# Select the Great Plains Level 1 ecoregion
eco_l2_semi_arid <- eco_l3 %>%
  filter(na_l2name == "WEST-CENTRAL SEMI-ARID PRAIRIES" |
         na_l2name == "SOUTH CENTRAL SEMI-ARID PRAIRIES")

# Plot with text labels
ggplot() +
  geom_sf(data = eco_l2_semi_arid,
          aes(fill = na_l3name),
          alpha = 0.5,
          show.legend = FALSE
          ) +
  geom_text(data = eco_l2_semi_arid,
            aes(x = text_x,
                y = text_y,
                label = na_l3name
                ),
            size = 3,
            color = "black",
            check_overlap = TRUE) +
  theme_minimal() +
  labs(title = "Map Title",
       fill = "Legend Title",
       caption = "Source: Your Source") +
  theme(legend.position = "none")

# save scratch plot
ggsave("figures/scratch.png",
       bg = "white")

```

## REDO BELOW

```{r 02_convert_to_spatial_reproject_data}

# convert stations into a spatial format (sf) object
sites_pk_bb <- st_as_sf(sites_pk_bb,
                    coords = c("dec_long_va",        # note x goes first
                                "dec_lat_va"),
                    crs = 4269,                     # projection, this is NAD83
                    remove = FALSE)                 # don't remove lat/lon cols

# check geometry of shapefiles
crs_geom <- eco_l3 %>%
  st_crs() %>%
  pluck(.,1)

crs_geom <- eco_l4 %>%
  st_crs() %>%
  pluck(.,1)

crs_geom <- state_bdy %>%
  st_crs() %>%
  pluck(.,1)

# reproject ecoregions to NAD83 Geographic (ESPG:4269) -- Lat Lon
eco_l3 <- eco_l3 %>%
  st_transform(crs = 4269)

eco_l4 <- eco_l4 %>%
  st_transform(crs = 4269)

```

```{r eval=FALSE}

eco_l3 %>%
  filter(na_l3name == "High Plains" |
         na_l3name == "Northwestern Great Plains" |
         na_l3name == "Nebraska Sand Hills") %>%
  ggplot() +
  geom_sf()




```

```{r 04_reproject_eco_shapefile}

# check geometry of ecoreg
crs_eco_l3 <- eco_l3 %>%
  st_crs()

# subset study area -- Albers refers to Albers Equal Area Conic projection
#   * High Plains
study_area_albers <- eco_l3 %>%
  filter(na_l3name == "High Plains" |
         na_l3name == "Northwestern Great Plains")

# project study area to NAD83 Geographic (ESPG:4269) -- Lat Lon
#   need to keep the study area
study_area_unproj <- study_area_albers %>%
  st_transform(crs = 4269)

# check results
crs_study_area_unproj <- study_area_unproj %>%
  st_crs()

```

```{r 05_reproject_eco_shapefile}
# make bounding box -- done recursively to create a bb in 01_get_ecoreg_shapefiles
bb_study_area_unproj <- study_area_unproj %>%
  st_bbox() %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  pivot_longer(-rowname) %>%
  pivot_wider(names_from = rowname,
              values_from = value) %>%
  select(-name) %>%
  mutate(delta_x = xmax - xmin) %>%
  mutate(delta_y = ymax - ymin)

# clean up Global Environment
rm(list = ls(pattern = "names"))
rm(list = ls(pattern = "crs"))
#rm(eco_l3)

```

```{r 06_intersect_sites_study_area}

# drop sites outside of ecoreg
sites_pk_in_ecoreg <- st_intersection(sites_pk_bb, study_area_unproj)

# filter sites with greater than 20 years of data
sites_gt_20 <- sites_pk_in_ecoreg %>%
  filter(count_nu >= 20) %>%
  distinct()

# check for duplicates
duplicates <- sites_gt_20 %>%
  filter(duplicated(.) | duplicated(., fromLast = TRUE))

# clean up Global Environment
rm(bb_study_area_unproj)

```

```{r 07_get_peak-flow_data_w_min_20_obs}

# get peak flow data for gages with min 20 observations
site_ids <- sites_gt_20 %>%
  select(site_no) %>%
  distinct()

peak_data_gt_20 <- readNWISpeak(site_ids$site_no)

# check results
ck_sites_gt_20 <- nrow(peak_data_gt_20 %>%
  group_by(site_no) %>%
  summarise(count = n())) == nrow(sites_gt_20)

duplicates <- peak_data_gt_20 %>%
  filter(duplicated(.) | duplicated(., fromLast = TRUE))

# remove duplicates
peak_data_no_dups <- peak_data_gt_20 %>%
  distinct()

# clean up Global Environment
rm(site_ids,
   ck_sites_gt_20,
   duplicates,
   peak_data_gt_20
   )

```

```{r 08_visually_check_results}

# plot results
ggplot() +
  geom_sf(data = study_area_unproj,
          alpha = 0.3) +
  geom_sf(data = sites_pk_bb,
          size  =  0.1,
          color = "gray80",
          alpha = 0.3) +
  geom_sf(data = sites_pk_in_ecoreg,
          size = 0.2,
          color = "red",
          alpha = 0.4) +
  geom_sf(data = sites_gt_20,
          size = 0.4,
          color = "black"
          ) +
  theme_bw() +
labs(
  title = "Initial gage selection",
  subtitle = "Peak flow gages with at least 20 years of record"
)

```

```{r 08_export_results}

# save plot of gages
ggsave("figures/gage-selection_initial.png")

# save site-data as a csv
write_csv(sites_pk_in_ecoreg, "data/sites_pk_in_ecoreg.csv")

write_csv(sites_gt_20, "data/sites_peak_gt_20.csv")

# save peak flow data as a csv
write_csv(peak_data_no_dups, "data/data_peak_gt_20.csv")

# Clean up Global Environment
rm(sites_pk_bb,
   sites_pk_in_ecoreg,
   study_area_albers,
   study_area_unproj,
   bb_study_area_unproj,
   )

```




```{r eval=FALSE}

#%>%
  filter(region != 9)   %>%     # eliminates island protectorates
  filter(division > 3)  %>%     # eliminates northeastern states
  filter(division != 5) %>%     # eliminates eastern states
  filter(division != 7) #%>%     # eliminates western states
  filter(stusps != "HI" &
         stusps != "GU" &
         stusps != "AS" &
         stusps != "MP" &
         stusps != "VI" &
         stusps != "AK" &
         stusps != "PR"
         )
```




