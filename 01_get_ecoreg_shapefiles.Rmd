---
title: "Get shapefiles for Northwestern Great Plains ecoregion gages"
author: "CJ Tinant"
date: "`r Sys.Date()`"
output: html_document
---

<!--
Shapefiles were downloaded locally and uploaded to ~/data-store/home/cjtinant/data_spatial
--> 

```{r 00_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# library
library(tidyverse)        # Load the 'Tidyverse' packages: ggplot2, dplyr, 
                          #   tidyr, readr, purrr, tibble, stringr, and forcats
library(dataRetrieval)    # Retrieval functions for USGS and EPA hydrology and
                          #   water quality data
library(sf)               # Simple features for R
library(here)             # A simpler way to find files
library(utils)            # The R utils package
library(XML)              # Tools for parsing and generating XML
library(xml2)             # Parse XML

```

```{r 01_dl_shapefiles_metadata_DOES-NOT-WORK-FROM-CYVERSE, eval=FALSE}

# 1. download shapefiles by calling the download.file() function, and passing 
#   in the URL and file name/location as arguments
# 2. make a new directory
# 3. unzip files to the new directory

# location where you want to save the file on your computer after here()
file_path <- "data_spatial/"

# level-3 ecoregion download----
# URL of the file to download and file name save on your computer
url       <- "https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l3.zip"
file_name <- "us_eco_l3.zip"
dir_name <- "us_eco_l3"

download.file(url, paste(file_path, file_name, sep = ""))
dir.create(paste(file_path, dir_name, sep = ""))
unzip(zipfile = paste(file_path, file_name, sep = ""),
      exdir = paste(file_path, dir_name, sep = ""))
file.remove(paste(file_path, file_name, sep = ""))

# level-4 ecoregion download----
# URL of the file to download and file name save on your computer
url <- "https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l4.zip"
file_name <- "us_eco_l4.zip"
dir_name <- "us_eco_l4"

download.file(url, paste(file_path, file_name, sep = ""))
dir.create(paste(file_path, dir_name, sep = ""))
unzip(zipfile = paste(file_path, file_name, sep = ""),
      exdir = paste(file_path, dir_name, sep = ""))
file.remove(paste(file_path, file_name, sep = ""))

# 2023 USDA Plant Hardiness Zone GIS datasets----
#   https://prism.oregonstate.edu/projects/plant_hardiness_zones.php
url <- "https://prism.oregonstate.edu/projects/phm_data/phzm_us_zones_shp_2023.zip"
file_name <- "phzm_us_zones_shp_2023.zip"
dir_name <- "phzm_us_zones_shp_2023"

download.file(url, paste(file_path, file_name, sep = ""))
dir.create(paste(file_path, dir_name, sep = ""))
unzip(zipfile = paste(file_path, file_name, sep = ""),
      exdir = paste(file_path, dir_name, sep = ""))
file.remove(paste(file_path, file_name, sep = ""))

# GRID data----
url <- "https://prism.oregonstate.edu/projects/phm_data/phzm_us_grid_2023.zip"
file_name <- "phzm_us_grid_2023.zip"
dir_name <- "phzm_us_grid_2023"

download.file(url, paste(file_path, file_name, sep = ""))
dir.create(paste(file_path, dir_name, sep = ""))
unzip(zipfile = paste(file_path, file_name, sep = ""),
      exdir = paste(file_path, dir_name, sep = ""))
file.remove(paste(file_path, file_name, sep = ""))

# USA State boundary----
# https://www.sciencebase.gov/catalog/item/52c78623e4b060b9ebca5be5
url <- "http://www2.census.gov/geo/tiger/TIGER2012/STATE/tl_2012_us_state.zip"
file_name <- "tl_2012_us_state.zip"
dir_name <- "tl_2012_us_state"

download.file(url, paste(file_path, file_name, sep = ""))
dir.create(paste(file_path, dir_name, sep = ""))
unzip(zipfile = paste(file_path, file_name, sep = ""),
      exdir = paste(file_path, dir_name, sep = ""))
file.remove(paste(file_path, file_name, sep = ""))

# USA Tribal Lands----
url <- "https://www2.census.gov/geo/tiger/TIGER2020/AITSN/tl_2020_us_aitsn.zip"
file_name <- "tl_2020_us_aitsn.zip"
dir_name <- "tl_2020_us_aitsn"

download.file(url, paste(file_path, file_name, sep = ""))
dir.create(paste(file_path, dir_name, sep = ""))
unzip(zipfile = paste(file_path, file_name, sep = ""),
      exdir = paste(file_path, dir_name, sep = ""))
file.remove(paste(file_path, file_name, sep = ""))

# get USA Tribal Lands metadata----
url <- "https://www2.census.gov/geo/docs/reference/codes2020/national_aiannh2020.txt"
file_path   <- "data_spatial/metadata/"
file_name   <- "national_aiannh2020.txt"
output_name <- "national_aiannh2020.csv"

# download file, read .txt, write .csv, remove file
download.file(url, paste(file_path, file_name, sep = ""))
data <- read_delim(paste(file_path, file_name, sep = ""),
                       delim = "|",
                       col_names = TRUE)
write_csv(data, paste(file_path, output_name, sep = ""))
file.remove(paste(file_path, file_name, sep = ""))

```

```{r 02_get_peak_flow_site_info}

# Get peak flow sites in the Northern Great Plains and High Plains bb
sites_nw <- whatNWISsites(                     #  xmin   ymin  xmax  ymax
  bBox = c(-112, 43.8, -105, 48.6)             #  west  south  east  north
  )

sites_ne <- whatNWISsites(
  bBox = c(-105, 43.8, -98.9, 48.6)
  )

sites_ncw <- whatNWISsites(                     #  xmin   ymin  xmax  ymax
  bBox = c(-112, 39.2, -105, 43.8)              #  west  south  east  north
  )

sites_nce <- whatNWISsites(
  bBox = c(-105, 39.2, -98.9, 43.8)
  )

sites_scw <- whatNWISsites(                     #  xmin   ymin  xmax  ymax
  bBox = c(-112, 35.6, -105, 39.2)              #  west  south  east  north
  )

sites_sce <- whatNWISsites(
  bBox = c(-105, 35.6, -98.9, 39.2)
  )

sites_sw <- whatNWISsites(                     #  xmin   ymin  xmax  ymax
  bBox = c(-112, 31.6, -105, 35.6)              #  west  south  east  north
  )

sites_se <- whatNWISsites(
  bBox = c(-105, 31.6, -98.9, 35.6)
  )

sites_sa <- whatNWISsites(
  bBox = c(-112, 31.2, -98.9, 31.6)
  )

# bind rows -all sites
sites_all <- bind_rows(sites_nw, sites_ne,
                       sites_ncw, sites_nce,
                       sites_scw, sites_sce,
                       sites_sw, sites_se,
                       sites_sa)

# clean up Global Environment
rm(list = ls(pattern = "sites_n"))
rm(list = ls(pattern = "sites_s"))

```

```{r 03_get_peak-flow_sites}

# get a list of all streamflow flow data in bounding box
sites_st <-sites_all %>%
  filter(site_tp_cd == "ST")

# get all peak flow data in bounding box
sites_pk_bb <-  whatNWISdata(
  siteNumber = sites_st$site_no,
  service = "pk"
  )

# write csv
write_csv(sites_pk_bb, "~/FFA_regional-skew/data/sites_pk_bb")

```

```{r 04_make_criteria_cde_metadata, eval=FALSE}
 
# the criteria code metadata is for peak flow sites
site_criteria_cde <- tribble( 
  ~code,
  ~description,
  "agency_cd", "The agency that is reporting the data. Agency codes are fixed values assigned by the National Water Information System (NWIS).",
  "site_no", "Each site in the USGS data base has a unique 8- to 15-digit identification number.",
  "station_nm", "This is the official name of the site in the database. For well information this can be a district-assigned local number.",
 "site_tp_cd", "A list of primary and secondary site types that can be associated with data collection sites. A site type is a generalized location in the hydrologic cycle, or a man-made feature thought to affect the hydrologic conditions measured at a site. All sites are associated with a primary site type, and may additionally be associated with a secondary site type that further describes the location.",
 "dec_lat_va", "Latitude in decimal degrees",
 "dec_long_va", "Longitude in decimal degrees",
 "coord_acy_cd", "Lat/Long coordinate accuracy codes indicating the accuracy of the latitude longitude values.",
 "dec_coord_datum_cd", "Lat/Long coordinate datum.",
 "alt_va", "Altitude of the site referenced to the specified Vertical Datum",
 "alt_acy_va", "Altitude accuracy value. Many altitudes are interpolated from the contours on topographic maps; accuracies determined in this way are generally entered as one-half of the contour interval.",
 "alt_datum_cd", "Altitude coordinate datum.",
 "huc_cd", "Hydrologic unit codes. The United States is divided and sub-divided into successively smaller hydrologic units which are classified into four levels: regions, sub-regions, accounting units, and cataloging units. The hydrologic units are arranged within each other, from the smallest (cataloging units) to the largest (regions). Each hydrologic unit is identified by a unique hydrologic unit code (HUC) consisting of two to eight digits based on the four levels of classification in the hydrologic unit system.",
 "data_type_cd", "All USGS data falls into one of: Current Conditions, Daily Data, Surface Water, Water Quality, Groundwater. Current condition data is any data down to the 15 minute interval that has been transmitted in the last 120 days. Daily Data is the average daily value for a site. Surface Water is water flow and levels in streams, lakes and springs. Water Quality is chemical and physical data for streams, lakes, springs, and wells. Groundwater is water levels in wells.",
 "parm_cd", "Parameter code",
 "stat_cd", "Statistics code",
 "ts_id", "Time-series ID. The ts_id is a surrogate key assigned and used by the database for efficient queries of data and for download or display as a list, table, or graph.",
 "loc_web_ds", "Additional measurement description",
 "medium_grp_cd", "Medium type refers to the specific environmental medium that was sampled and analyzed. Medium type differs from site type because one site type, such as surface water, could have data for several media, such as water, bottom sediment, fish tissue, and others.",
 "parm_grp_cd", "Parameter group code",
 "srs_id", "USEPA SRS.http://www.epa.gov/srs/",
 "access_cd", "Access code",
 "begin_date", "Begin date of the period of record for the data",
 "end_date", "End date of the period of record for the data",
 "count_nu", "Number of records",
 "na_l3_cde", "User-generated code for level-3 ecoregion names, where 'ngp' = 'Northern Great Plains' and 'hp' = High Plains"
 )


# write metadata file ---
write_csv(site_criteria_cde, "data_output/sites_site_criteria_cde.csv")

# clean Global Environment
rm(list = ls(pattern = "_cde"))

```


```{r xx_read_XML_data_NOT-WORKING, eval=FALSE}
# discuss this
# Prepare to read XML metadata from a file----
file_path <- "data_spatial/tl_2020_us_aitsn/"
file_name <- "tl_2020_us_aitsn.shp.ea.iso.xml"

# Read the XML Data----
xml_data <- read_xml(paste(file_path, file_name, sep = ""))

# Extracting elements using an example XPath, adjust the XPath to fit your XML structure
nodes <- xml_find_all(xml_data, "//your_element", ns = xml_ns(xml_data))

# Extracting text from the nodes
texts <- xml_text(nodes)

# Printing out the texts to see what was extracted
print(texts)

# Assuming XML data is stored in a variable called xml_data
xml_data <- read_xml('<?xml version="1.0" encoding="UTF-8"?>
<!-- your XML content here -->')

Step 3: Extract and Format Data

You can extract elements and their attributes using xml_find_all and xml_attr. To make it human-readable, you might format it into a tidy data frame:

R

# Example of extracting and formatting data
metadata <- xml_data %>%
  xml_find_all(".//gfc:FC_FeatureAttribute", ns = xml_ns(xml_data)) %>%
  map_df(~{
    data_frame(
      Name = xml_text(xml_find_first(.x, ".//gco:LocalName", ns = xml_ns(xml_data))),
      Definition = xml_text(xml_find_first(.x, ".//gco:CharacterString", ns = xml_ns(xml_data)))
    )
  })

Step 4: Display or Save the Data

Once you have the data in a data frame, you can easily print it out or save it to a more convenient format such as CSV:

R

print(metadata)

# Save to CSV
write.csv(metadata, "metadata.csv", row.names = FALSE)

Note:

    Make sure that the XPath queries match the structure of your XML. You might need to adjust the paths based on your specific XML structure and the namespaces involved.
    Replace "<!-- your XML content here -->" with your actual XML content or adapt the script to load from a file or URL as needed.
    
    
    
xml_data <- xmlTreeParse(paste(file_path, file_name, sep = ""),
                         useInternalNodes = TRUE)

xml_data
```






